from __gin__ import dynamic_registration

import __main__ as train_script
from t5x import models
from t5x import partitioning
from t5x import trainer
from t5x import utils
import seqio

from hyper_task_descriptions import t0_eval_mixtures # To import seqio task

MIXTURE_OR_TASK_MODULE = "t5.data.mixtures"
TRAIN_STEPS = 1117200 # 1112200 (pretrain) + 5000 (finetune)
DROPOUT_RATE = 0.1
BATCH_SIZE = 2048

train/utils.DatasetConfig:
  batch_size = %BATCH_SIZE
  use_cached = True
  pack = True
  use_custom_packing_ops = False

train_eval/utils.DatasetConfig:
  batch_size = %BATCH_SIZE
  use_cached = True
  pack = True
  use_custom_packing_ops = False

train_script.train:
  eval_period = 500
  eval_steps = 50
  random_seed = None
  infer_eval_dataset_cfg = None # Prevent to run inference evaluation
  train_eval_dataset_cfg = None # Prevent to run evaluation as it seems to OOM me.

utils.create_learning_rate_scheduler:
  base_learning_rate = 0.001
  warmup_steps = 10000 # irrelevant

utils.SaveCheckpointConfig:
  period = 1000  # checkpoint frequency
  keep = None # only keep one checkpoint

utils.RestoreCheckpointConfig:
  strict = False
  dtype = 'bfloat16'

partitioning.PjitPartitioner.num_partitions = 2

trainer.Trainer.num_microbatches = 32 # 2048 // 32

seqio.Evaluator.use_memory_cache = False
