# T5.1.1 Base model.
from __gin__ import dynamic_registration

from flax import optim
from t5x import adafactor
from t5x import optimizers
from t5x import utils
from hyper_task_descriptions import utils as hyper_utils

# gin that allows partial training based on regex matching.

# ------------------- Partial loading ------------------------------------------------
OPTIMIZER = @optimizers.MultiOptimizer()
optimizers.MultiOptimizer:
  traversals_and_optimizers = ((@optim.ModelParamTraversal(),
                                @adafactor.Adafactor()),)
optim.ModelParamTraversal:
  filter_fn = @hyper_utils.match_any()
# MultiOptimizer will match any parameter with a flattened name that
# matches any of these regular expressions.
PROMPT_REGEX = [".*/hyper/.*"]
# PROMPT_REGEX = ["^((?!roberta).)*$"]
hyper_utils.match_any.regexes = %PROMPT_REGEX

# These setting allow us to partially reload a checkpoint, that is, we can load
# most of the model weights from the checkpoint, without it complaining that we
# don't have a weight for our prompt in the checkpoint.
utils.RestoreCheckpointConfig:
  # Activate the codepath that allow of the merging of the optimizer state as
  # specified in the config (with our new parameter) and the optimizer state as
  # defined in the checkpoint.
  fallback_to_scratch = True
  # Use the T5X assignment map to grab values from the checkpoint. Each entry in
  # the map is a regular expression that matches some flatten variable in the
  # optimizer state as defined in the model created by the config. The second
  # value is the corresponding name in optimizer state as defined by the
  # checkpoint. It supports interpolating capture groups from the initial regex.
  # If the second pattern it `None` we skip trying to load this variable from
  # the checkpoint.

  # We skip hypernetwork parameters
  assignment_map = (
      (r"^.*hyper.*$", None),
      )