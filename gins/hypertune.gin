from __gin__ import dynamic_registration

from t5x import utils
from hyper_task_descriptions.modeling import hyper_network

# hypertune: decoder in hypernet
hyper_network.HyperTransformer.config = @hyper_network.HyperT5Config()
hyper_network.HyperT5Config:
  use_adapter = False
  use_prefix = True
  use_fusion_in_decoder = False
  layer_embedding_method = "decoder"

# we restore hypernetwork weights from pretrained model
utils.RestoreCheckpointConfig:
  fallback_to_scratch = True
  assignment_map = (
      # for some reason, we get non-partitioned dict entries. this hacks this by mapping
      # them to none. I suspect this means the hypernet has its optimizer states reset...
      ('(.*)/param_states/(encoder|hyper_encoder)/(.*)/(scale|kernel|rel_embedding)$', None),
      ('(.*)/param_states/(decoder|hyper_decoder)/(.*)/(scale|kernel|rel_embedding)$', None),
      # regular restore, using groups
      ('(.*)/(hyper/hyper_encoder|encoder)/(.*)', r'\1/encoder/\3'),
      ('(.*)/(hyper/hyper_decoder|decoder)/(.*)', r'\1/decoder/\3'),
      # the non-t5 bits of hypernet need to be initialised from scratch
      ('.*hyper/[^h].*', None),
    )