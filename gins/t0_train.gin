# For training T0 (xxl = 11b, xl = 3b). Make sure you have cached p3 first!
from __gin__ import dynamic_registration

from t5x import models
from t5x import trainer
from t5x import utils
import seqio
from hyper_task_descriptions.seqio_tasks import all_t0_tasks

import __main__ as train_script

include "t5x/configs/runs/finetune.gin"
include "gins/t0.gin" # This overrides some default config in `t5x/configs/runs/finetune.gin`

TASK_FEATURE_LENGTHS = {"inputs": 1024, "hyper_inputs": 512, "task_names": 1, "targets": 256}
MIXTURE_OR_TASK_NAME = "t0_train"

trainer.Trainer.num_microbatches = 16 # 2048 // 16
trainer.Trainer.weight_metrics_computer = @trainer.WeightMetricsComputer()

# the batch sizes are really big so I increase stats logging
# for more visibility.
train_script.train:
    stats_period = 100 # default is eval_period, which is 1000
