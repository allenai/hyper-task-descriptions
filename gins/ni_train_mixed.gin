# For training T0 (xxl = 11b, xl = 3b). Make sure you have cached p3 first!
from __gin__ import dynamic_registration

from t5x import models
from t5x import trainer
from t5x import utils
import seqio
from hyper_task_descriptions.c4 import c4_ni_registry

import __main__ as train_script

include "t5x/configs/runs/finetune.gin"
include "gins/t0.gin" # This overrides some default config in `t5x/configs/runs/finetune.gin`
include "gins/restore_pretrained.gin" # for loading from checkpoints

TASK_FEATURE_LENGTHS = {"inputs": 1024, "hyper_inputs": 1024, "task_names": 1, "targets": 256}
MIXTURE_OR_TASK_NAME = "c4_ni"

trainer.Trainer.num_microbatches = 16 # 2048 // 16
trainer.Trainer.weight_metrics_computer = @trainer.WeightMetricsComputer()

# the batch sizes are really big so I increase stats logging
# for more visibility.
train_script.train:
    stats_period = 50 # default is eval_period, which is 1000


utils.SaveCheckpointConfig:
  period = 100  # checkpoint frequency
