# T5.1.1 Base model.
from __gin__ import dynamic_registration

from t5x import utils


# These setting allow us to partially reload a checkpoint, that is, we can load
# most of the model weights from the checkpoint, without it complaining that we
# don't have a weight for our prompt in the checkpoint.
utils.RestoreCheckpointConfig:
  # Activate the codepath that allow of the merging of the optimizer state as
  # specified in the config (with our new parameter) and the optimizer state as
  # defined in the checkpoint.
  fallback_to_scratch = False
  # Use the T5X assignment map to grab values from the checkpoint. Each entry in
  # the map is a regular expression that matches some flatten variable in the
  # optimizer state as defined in the model created by the config. The second
  # value is the corresponding name in optimizer state as defined by the
  # checkpoint. It supports interpolating capture groups from the initial regex.
  # If the second pattern it `None` we skip trying to load this variable from
  # the checkpoint.

  # reset just the optimizer for now.
  assignment_map = (
    (r".*inner_states.*", None),
    (r".*count.*", None),
  )
