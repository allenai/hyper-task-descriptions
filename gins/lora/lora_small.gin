# T5.1.1 small model.

include 'gins/lora/lora_base.gin'  # imports vocab, optimizer and model.

# ------------------- Network specification overrides --------------------------
lora_network.LoraTransformer.config = @hyper_network.HyperT5Config()
hyper_network.HyperT5Config:
  emb_dim = 512
  num_heads = 6
  num_encoder_layers = 8
  num_decoder_layers = 8
  head_dim = 64
  mlp_dim = 1024
  adapter_size = 8
  num_prefix_tokens = 7
  hbottleneck_size = 16
  roberta_model = "hamishivi/fixed-distilroberta-base"
  lora_ranks = (2, None, 2, None)


