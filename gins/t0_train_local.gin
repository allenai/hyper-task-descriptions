# For training T0, but locally (using tiny model and training from scratch on a small set)
# use this to test your modeling code - you'll need a gin that adds your modeling code (it should be compat. with t5 gins)
from __gin__ import dynamic_registration

from t5x import trainer
from t5x import partitioning
from t5x import utils
import seqio
from hyper_task_descriptions.seqio_tasks import small_t0_tasks

import __main__ as train_script

# include "t5x/examples/t5/t5_1_1/tiny.gin"
# include "t5x/configs/runs/finetune.gin"
include "gins/finetune_from_scratch.gin"
include "gins/hyper_tiny.gin"
include "gins/t0.gin"
include "gins/partial_train.gin"

# 'task_names' is a string, so no length.
TASK_FEATURE_LENGTHS = {"inputs": 8, "hyper_inputs": 8, "task_names": 1, "targets": 7}
MIXTURE_OR_TASK_NAME = "t0_small_train"
BATCH_SIZE = 4
TRAIN_STEPS = 1100010
DROPOUT_RATE = 0.1

train_script.train.random_seed = 42  # dropout seed
train/utils.DatasetConfig.seed = 42  # dataset seed

train/utils.DatasetConfig:
  batch_size = 8
  shuffle = False

train_eval/utils.DatasetConfig.batch_size = 8

train_script.train:
  eval_period = 3
  eval_steps = 3
  stats_period = 1

trainer.Trainer.num_microbatches = 2
partitioning.PjitPartitioner:
  num_partitions = 1
  model_parallel_submesh = None

# utils.CheckpointConfig:
#   restore = None

infer_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS

# DISABLE INFERENCE EVAL
# train_script.train.infer_eval_dataset_cfg = None
